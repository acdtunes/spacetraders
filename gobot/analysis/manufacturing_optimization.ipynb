{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manufacturing Operation Data Analytics\n",
    "\n",
    "## Objective\n",
    "Analyze manufacturing operation data to discover optimizations, hidden correlations, and improve decision-making.\n",
    "\n",
    "## Key Questions\n",
    "1. Should we use market activity (WEAK/GROWING/STRONG/RESTRICTED) in decisions?\n",
    "2. Are our position sizing multipliers optimal?\n",
    "3. What hidden correlations exist in our data?\n",
    "4. Can we predict supply level changes?\n",
    "5. What other optimizations can we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Extraction from PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "# Update these credentials to match your .env\n",
    "DB_HOST = '127.0.0.1'\n",
    "DB_PORT = '5432'\n",
    "DB_NAME = 'spacetraders'\n",
    "DB_USER = 'spacetraders'\n",
    "DB_PASSWORD = 'dev_password'\n",
    "PLAYER_ID = 12  # Update to your player ID\n",
    "\n",
    "engine = create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}')\n",
    "\n",
    "print(f'Connected to {DB_NAME} database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market price history\n",
    "query_price_history = f\"\"\"\n",
    "SELECT \n",
    "    waypoint_symbol, good_symbol,\n",
    "    purchase_price, sell_price, \n",
    "    supply, activity, trade_volume,\n",
    "    recorded_at\n",
    "FROM market_price_history\n",
    "WHERE player_id = {PLAYER_ID}\n",
    "ORDER BY recorded_at DESC\n",
    "\"\"\"\n",
    "df_prices = pd.read_sql(query_price_history, engine)\n",
    "df_prices['recorded_at'] = pd.to_datetime(df_prices['recorded_at'])\n",
    "print(f'Market price history: {len(df_prices):,} records')\n",
    "print(f'Date range: {df_prices[\"recorded_at\"].min()} to {df_prices[\"recorded_at\"].max()}')\n",
    "df_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transactions (ledger)\n",
    "query_transactions = f\"\"\"\n",
    "SELECT \n",
    "    id, timestamp, transaction_type, category, \n",
    "    amount, balance_before, balance_after,\n",
    "    description, metadata, operation_type,\n",
    "    related_entity_type, related_entity_id,\n",
    "    created_at\n",
    "FROM transactions\n",
    "WHERE player_id = {PLAYER_ID}\n",
    "ORDER BY created_at DESC\n",
    "\"\"\"\n",
    "df_transactions = pd.read_sql(query_transactions, engine)\n",
    "df_transactions['created_at'] = pd.to_datetime(df_transactions['created_at'])\n",
    "print(f'Transactions: {len(df_transactions):,} records')\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manufacturing tasks\n",
    "query_tasks = f\"\"\"\n",
    "SELECT \n",
    "    id, pipeline_id, task_type, status, good, quantity, actual_quantity,\n",
    "    source_market, target_market, factory_symbol,\n",
    "    assigned_ship, priority, retry_count, max_retries,\n",
    "    total_cost, total_revenue, error_message,\n",
    "    created_at, ready_at, started_at, completed_at\n",
    "FROM manufacturing_tasks\n",
    "WHERE player_id = {PLAYER_ID}\n",
    "ORDER BY created_at DESC\n",
    "\"\"\"\n",
    "df_tasks = pd.read_sql(query_tasks, engine)\n",
    "for col in ['created_at', 'ready_at', 'started_at', 'completed_at']:\n",
    "    df_tasks[col] = pd.to_datetime(df_tasks[col])\n",
    "print(f'Manufacturing tasks: {len(df_tasks):,} records')\n",
    "df_tasks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manufacturing pipelines\n",
    "query_pipelines = f\"\"\"\n",
    "SELECT \n",
    "    id, sequence_number, pipeline_type, product_good, sell_market,\n",
    "    expected_price, status, total_cost, total_revenue, net_profit,\n",
    "    error_message, created_at, started_at, completed_at\n",
    "FROM manufacturing_pipelines\n",
    "WHERE player_id = {PLAYER_ID}\n",
    "ORDER BY created_at DESC\n",
    "\"\"\"\n",
    "df_pipelines = pd.read_sql(query_pipelines, engine)\n",
    "for col in ['created_at', 'started_at', 'completed_at']:\n",
    "    df_pipelines[col] = pd.to_datetime(df_pipelines[col])\n",
    "print(f'Manufacturing pipelines: {len(df_pipelines):,} records')\n",
    "df_pipelines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load current market data\n",
    "query_market = f\"\"\"\n",
    "SELECT \n",
    "    waypoint_symbol, good_symbol, supply, activity,\n",
    "    purchase_price, sell_price, trade_volume, trade_type,\n",
    "    last_updated\n",
    "FROM market_data\n",
    "WHERE player_id = {PLAYER_ID}\n",
    "\"\"\"\n",
    "df_market = pd.read_sql(query_market, engine)\n",
    "df_market['last_updated'] = pd.to_datetime(df_market['last_updated'])\n",
    "print(f'Current market data: {len(df_market):,} records')\n",
    "df_market.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load container logs\n",
    "query_logs = f\"\"\"\n",
    "SELECT \n",
    "    container_id, timestamp, level, message, metadata\n",
    "FROM container_logs\n",
    "WHERE player_id = {PLAYER_ID}\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 10000\n",
    "\"\"\"\n",
    "df_logs = pd.read_sql(query_logs, engine)\n",
    "df_logs['timestamp'] = pd.to_datetime(df_logs['timestamp'])\n",
    "print(f'Container logs: {len(df_logs):,} records')\n",
    "df_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for price history\n",
    "print('=== Price History Summary ===')\n",
    "print(f'Unique waypoints: {df_prices[\"waypoint_symbol\"].nunique()}')\n",
    "print(f'Unique goods: {df_prices[\"good_symbol\"].nunique()}')\n",
    "print(f'\\nSupply distribution:')\n",
    "print(df_prices['supply'].value_counts())\n",
    "print(f'\\nActivity distribution:')\n",
    "print(df_prices['activity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize supply and activity distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Supply distribution\n",
    "supply_order = ['SCARCE', 'LIMITED', 'MODERATE', 'HIGH', 'ABUNDANT']\n",
    "supply_counts = df_prices['supply'].value_counts().reindex(supply_order).fillna(0)\n",
    "axes[0].bar(supply_counts.index, supply_counts.values, color='steelblue')\n",
    "axes[0].set_title('Supply Level Distribution', fontsize=14)\n",
    "axes[0].set_xlabel('Supply Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Activity distribution\n",
    "activity_order = ['WEAK', 'RESTRICTED', 'GROWING', 'STRONG']\n",
    "activity_counts = df_prices['activity'].value_counts().reindex(activity_order).fillna(0)\n",
    "axes[1].bar(activity_counts.index, activity_counts.values, color='coral')\n",
    "axes[1].set_title('Activity Level Distribution', fontsize=14)\n",
    "axes[1].set_xlabel('Activity Level')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/supply_activity_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction summary\n",
    "print('=== Transaction Summary ===')\n",
    "print(f'\\nBy transaction type:')\n",
    "print(df_transactions.groupby('transaction_type')['amount'].agg(['count', 'sum', 'mean']))\n",
    "print(f'\\nBy category:')\n",
    "print(df_transactions.groupby('category')['amount'].agg(['count', 'sum', 'mean']))\n",
    "print(f'\\nBy operation type:')\n",
    "print(df_transactions.groupby('operation_type')['amount'].agg(['count', 'sum', 'mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task summary\n",
    "print('=== Task Summary ===')\n",
    "print(f'\\nBy task type:')\n",
    "print(df_tasks.groupby('task_type')[['total_cost', 'total_revenue']].agg(['count', 'sum', 'mean']))\n",
    "print(f'\\nBy status:')\n",
    "print(df_tasks['status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline profitability summary\n",
    "print('=== Pipeline Summary ===')\n",
    "completed_pipelines = df_pipelines[df_pipelines['status'] == 'COMPLETED']\n",
    "print(f'Completed pipelines: {len(completed_pipelines)}')\n",
    "if len(completed_pipelines) > 0:\n",
    "    print(f'\\nProfitability by product:')\n",
    "    print(completed_pipelines.groupby('product_good')[['total_cost', 'total_revenue', 'net_profit']].agg(['count', 'sum', 'mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Market Activity Analysis (PRIORITY - Currently Unused!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H1: Does activity level correlate with price changes?\n",
    "print('=== H1: Market Activity vs Price Changes ===')\n",
    "\n",
    "# Calculate price changes for each waypoint-good combination\n",
    "df_prices_sorted = df_prices.sort_values(['waypoint_symbol', 'good_symbol', 'recorded_at'])\n",
    "df_prices_sorted['price_change'] = df_prices_sorted.groupby(['waypoint_symbol', 'good_symbol'])['purchase_price'].diff()\n",
    "df_prices_sorted['price_change_pct'] = df_prices_sorted.groupby(['waypoint_symbol', 'good_symbol'])['purchase_price'].pct_change() * 100\n",
    "\n",
    "# Analyze price changes by activity level\n",
    "activity_price_stats = df_prices_sorted.groupby('activity').agg({\n",
    "    'price_change': ['mean', 'std', 'count'],\n",
    "    'price_change_pct': ['mean', 'std'],\n",
    "    'purchase_price': 'mean'\n",
    "}).round(2)\n",
    "print('\\nPrice change statistics by activity level:')\n",
    "print(activity_price_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price volatility by activity level\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot of price changes by activity\n",
    "activity_data = df_prices_sorted[df_prices_sorted['activity'].notna()]\n",
    "if len(activity_data) > 0:\n",
    "    activity_order = ['WEAK', 'RESTRICTED', 'GROWING', 'STRONG']\n",
    "    available_activities = [a for a in activity_order if a in activity_data['activity'].unique()]\n",
    "    \n",
    "    sns.boxplot(data=activity_data, x='activity', y='price_change_pct', \n",
    "                order=available_activities, ax=axes[0])\n",
    "    axes[0].set_title('Price Change % by Activity Level', fontsize=14)\n",
    "    axes[0].set_xlabel('Activity Level')\n",
    "    axes[0].set_ylabel('Price Change %')\n",
    "    axes[0].set_ylim(-50, 50)  # Clip outliers for visibility\n",
    "\n",
    "    # Mean price by activity\n",
    "    mean_prices = activity_data.groupby('activity')['purchase_price'].mean().reindex(available_activities)\n",
    "    axes[1].bar(mean_prices.index, mean_prices.values, color='steelblue')\n",
    "    axes[1].set_title('Mean Price by Activity Level', fontsize=14)\n",
    "    axes[1].set_xlabel('Activity Level')\n",
    "    axes[1].set_ylabel('Mean Purchase Price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/activity_price_analysis.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test: Do different activity levels have significantly different price volatility?\n",
    "print('=== Statistical Test: Activity vs Price Volatility ===')\n",
    "\n",
    "# ANOVA test for price change variance across activity levels\n",
    "activity_groups = [group['price_change_pct'].dropna() for name, group in \n",
    "                   df_prices_sorted.groupby('activity') if len(group) > 10]\n",
    "\n",
    "if len(activity_groups) >= 2:\n",
    "    f_stat, p_value = stats.f_oneway(*activity_groups)\n",
    "    print(f'\\nANOVA F-statistic: {f_stat:.4f}')\n",
    "    print(f'P-value: {p_value:.6f}')\n",
    "    if p_value < 0.05:\n",
    "        print('SIGNIFICANT: Activity levels have different price volatility!')\n",
    "    else:\n",
    "        print('Not significant: Activity levels have similar price volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity-Supply relationship\n",
    "print('=== Activity-Supply Relationship ===')\n",
    "\n",
    "# Cross-tabulation\n",
    "activity_supply_crosstab = pd.crosstab(df_prices['activity'], df_prices['supply'], normalize='all') * 100\n",
    "print('\\nActivity-Supply cross-tabulation (%):')\n",
    "print(activity_supply_crosstab.round(2))\n",
    "\n",
    "# Chi-square test for independence\n",
    "contingency = pd.crosstab(df_prices['activity'], df_prices['supply'])\n",
    "if contingency.shape[0] > 1 and contingency.shape[1] > 1:\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "    print(f'\\nChi-square test for independence:')\n",
    "    print(f'Chi2: {chi2:.4f}, P-value: {p_value:.6f}')\n",
    "    if p_value < 0.05:\n",
    "        print('SIGNIFICANT: Activity and Supply are NOT independent!')\n",
    "    else:\n",
    "        print('Not significant: Activity and Supply appear independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Position Sizing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2: Are position sizing multipliers optimal?\n",
    "# Current hardcoded values: ABUNDANT=0.8, HIGH=0.6, MODERATE=0.4, LIMITED=0.2, SCARCE=0.1\n",
    "print('=== H2: Position Sizing Analysis ===')\n",
    "\n",
    "# Parse metadata from transactions to get purchase details\n",
    "import json\n",
    "\n",
    "def parse_metadata(x):\n",
    "    if pd.isna(x) or x == '':\n",
    "        return {}\n",
    "    try:\n",
    "        if isinstance(x, dict):\n",
    "            return x\n",
    "        return json.loads(x)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "df_transactions['metadata_parsed'] = df_transactions['metadata'].apply(parse_metadata)\n",
    "df_transactions['good'] = df_transactions['metadata_parsed'].apply(lambda x: x.get('good', x.get('symbol', None)))\n",
    "df_transactions['quantity'] = df_transactions['metadata_parsed'].apply(lambda x: x.get('quantity', x.get('units', None)))\n",
    "df_transactions['waypoint'] = df_transactions['metadata_parsed'].apply(lambda x: x.get('waypoint', None))\n",
    "\n",
    "print(f'Transactions with good info: {df_transactions[\"good\"].notna().sum()}')\n",
    "print(f'Transactions with quantity info: {df_transactions[\"quantity\"].notna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze purchase transactions - join with market conditions at purchase time\n",
    "purchase_txns = df_transactions[(df_transactions['transaction_type'] == 'DEBIT') & \n",
    "                                 (df_transactions['category'] == 'CARGO_TRADE')].copy()\n",
    "print(f'\\nPurchase transactions: {len(purchase_txns)}')\n",
    "\n",
    "if len(purchase_txns) > 0:\n",
    "    # Calculate per-unit cost\n",
    "    purchase_txns['quantity'] = pd.to_numeric(purchase_txns['quantity'], errors='coerce')\n",
    "    purchase_txns['per_unit_cost'] = abs(purchase_txns['amount']) / purchase_txns['quantity'].replace(0, np.nan)\n",
    "    \n",
    "    print('\\nPurchase statistics by good:')\n",
    "    purchase_stats = purchase_txns.groupby('good').agg({\n",
    "        'amount': ['count', 'sum'],\n",
    "        'quantity': ['sum', 'mean'],\n",
    "        'per_unit_cost': 'mean'\n",
    "    }).round(2)\n",
    "    print(purchase_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sale transactions - calculate profitability\n",
    "sale_txns = df_transactions[(df_transactions['transaction_type'] == 'CREDIT') & \n",
    "                            (df_transactions['category'] == 'CARGO_TRADE')].copy()\n",
    "print(f'Sale transactions: {len(sale_txns)}')\n",
    "\n",
    "if len(sale_txns) > 0:\n",
    "    sale_txns['quantity'] = pd.to_numeric(sale_txns['quantity'], errors='coerce')\n",
    "    sale_txns['per_unit_revenue'] = sale_txns['amount'] / sale_txns['quantity'].replace(0, np.nan)\n",
    "    \n",
    "    print('\\nSale statistics by good:')\n",
    "    sale_stats = sale_txns.groupby('good').agg({\n",
    "        'amount': ['count', 'sum'],\n",
    "        'quantity': ['sum', 'mean'],\n",
    "        'per_unit_revenue': 'mean'\n",
    "    }).round(2)\n",
    "    print(sale_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position size vs profitability regression\n",
    "# Join completed tasks with their pipelines to get full picture\n",
    "completed_tasks = df_tasks[df_tasks['status'] == 'COMPLETED'].copy()\n",
    "completed_tasks['profit'] = completed_tasks['total_revenue'] - completed_tasks['total_cost']\n",
    "completed_tasks['profit_margin'] = completed_tasks['profit'] / completed_tasks['total_cost'].replace(0, np.nan) * 100\n",
    "\n",
    "print('=== Task Profitability by Type ===')\n",
    "task_profit = completed_tasks.groupby('task_type').agg({\n",
    "    'profit': ['count', 'sum', 'mean'],\n",
    "    'profit_margin': 'mean',\n",
    "    'actual_quantity': 'mean'\n",
    "}).round(2)\n",
    "print(task_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantity vs profit relationship\n",
    "if len(completed_tasks) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter plot: quantity vs profit\n",
    "    valid_data = completed_tasks[(completed_tasks['actual_quantity'] > 0) & \n",
    "                                  (completed_tasks['profit'].notna())]\n",
    "    if len(valid_data) > 0:\n",
    "        axes[0].scatter(valid_data['actual_quantity'], valid_data['profit'], alpha=0.5)\n",
    "        axes[0].set_xlabel('Quantity')\n",
    "        axes[0].set_ylabel('Profit')\n",
    "        axes[0].set_title('Quantity vs Profit')\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(valid_data['actual_quantity'], valid_data['profit'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(valid_data['actual_quantity'].min(), valid_data['actual_quantity'].max(), 100)\n",
    "        axes[0].plot(x_line, p(x_line), 'r--', label=f'Trend: y={z[0]:.2f}x+{z[1]:.0f}')\n",
    "        axes[0].legend()\n",
    "    \n",
    "    # Box plot: profit by task type\n",
    "    task_types_with_data = completed_tasks.groupby('task_type')['profit'].count()\n",
    "    valid_task_types = task_types_with_data[task_types_with_data > 5].index.tolist()\n",
    "    if valid_task_types:\n",
    "        sns.boxplot(data=completed_tasks[completed_tasks['task_type'].isin(valid_task_types)], \n",
    "                    x='task_type', y='profit', ax=axes[1])\n",
    "        axes[1].set_title('Profit Distribution by Task Type')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/figures/quantity_profit_analysis.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix for correlation analysis\n",
    "print('=== Creating Feature Matrix ===')\n",
    "\n",
    "# Encode categorical variables\n",
    "supply_order = {'SCARCE': 1, 'LIMITED': 2, 'MODERATE': 3, 'HIGH': 4, 'ABUNDANT': 5}\n",
    "activity_order = {'WEAK': 1, 'RESTRICTED': 2, 'GROWING': 3, 'STRONG': 4}\n",
    "\n",
    "df_features = df_prices_sorted.copy()\n",
    "df_features['supply_encoded'] = df_features['supply'].map(supply_order)\n",
    "df_features['activity_encoded'] = df_features['activity'].map(activity_order)\n",
    "\n",
    "# Calculate additional features\n",
    "df_features['spread'] = df_features['sell_price'] - df_features['purchase_price']\n",
    "df_features['spread_pct'] = df_features['spread'] / df_features['purchase_price'] * 100\n",
    "df_features['hour'] = df_features['recorded_at'].dt.hour\n",
    "\n",
    "# Select numeric columns for correlation\n",
    "numeric_cols = ['purchase_price', 'sell_price', 'trade_volume', 'supply_encoded', \n",
    "                'activity_encoded', 'spread', 'spread_pct', 'price_change', \n",
    "                'price_change_pct', 'hour']\n",
    "df_numeric = df_features[numeric_cols].dropna()\n",
    "print(f'Feature matrix: {len(df_numeric)} rows, {len(numeric_cols)} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "if len(df_numeric) > 10:\n",
    "    correlation_matrix = df_numeric.corr()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "                square=True, linewidths=0.5)\n",
    "    plt.title('Feature Correlation Matrix', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/figures/correlation_heatmap.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strongest correlations\n",
    "    print('\\n=== Strongest Correlations (|r| > 0.3) ===')\n",
    "    corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            r = correlation_matrix.iloc[i, j]\n",
    "            if abs(r) > 0.3:\n",
    "                corr_pairs.append((correlation_matrix.columns[i], \n",
    "                                   correlation_matrix.columns[j], r))\n",
    "    \n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    for col1, col2, r in corr_pairs:\n",
    "        print(f'{col1} <-> {col2}: r={r:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation for non-linear relationships\n",
    "if len(df_numeric) > 10:\n",
    "    spearman_corr = df_numeric.corr(method='spearman')\n",
    "    \n",
    "    print('\\n=== Spearman Correlations (rank-based, captures non-linear) ===')\n",
    "    spearman_pairs = []\n",
    "    for i in range(len(spearman_corr.columns)):\n",
    "        for j in range(i+1, len(spearman_corr.columns)):\n",
    "            r = spearman_corr.iloc[i, j]\n",
    "            if abs(r) > 0.3:\n",
    "                spearman_pairs.append((spearman_corr.columns[i], \n",
    "                                       spearman_corr.columns[j], r))\n",
    "    \n",
    "    spearman_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    for col1, col2, r in spearman_pairs:\n",
    "        print(f'{col1} <-> {col2}: rho={r:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Supply Transition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H5: How long does supply stay at each level? Are transitions predictable?\n",
    "print('=== H5: Supply Transition Analysis ===')\n",
    "\n",
    "# Calculate supply transitions\n",
    "df_supply = df_prices_sorted.copy()\n",
    "df_supply['prev_supply'] = df_supply.groupby(['waypoint_symbol', 'good_symbol'])['supply'].shift(1)\n",
    "df_supply['supply_changed'] = df_supply['supply'] != df_supply['prev_supply']\n",
    "df_supply['time_diff'] = df_supply.groupby(['waypoint_symbol', 'good_symbol'])['recorded_at'].diff().dt.total_seconds() / 60  # in minutes\n",
    "\n",
    "# Supply transition matrix\n",
    "transitions = df_supply[df_supply['supply_changed'] & df_supply['prev_supply'].notna()]\n",
    "transition_matrix = pd.crosstab(transitions['prev_supply'], transitions['supply'], normalize='index') * 100\n",
    "print('\\nSupply Transition Probabilities (%):')\n",
    "print(transition_matrix.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition matrix as heatmap\n",
    "if len(transition_matrix) > 0:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    supply_order_display = ['SCARCE', 'LIMITED', 'MODERATE', 'HIGH', 'ABUNDANT']\n",
    "    available_supplies = [s for s in supply_order_display if s in transition_matrix.index]\n",
    "    \n",
    "    if available_supplies:\n",
    "        matrix_ordered = transition_matrix.reindex(index=available_supplies, columns=available_supplies).fillna(0)\n",
    "        sns.heatmap(matrix_ordered, annot=True, fmt='.1f', cmap='Blues',\n",
    "                    xticklabels=available_supplies, yticklabels=available_supplies)\n",
    "        plt.title('Supply Level Transition Probabilities (%)', fontsize=14)\n",
    "        plt.xlabel('Next Supply Level')\n",
    "        plt.ylabel('Current Supply Level')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('outputs/figures/supply_transition_matrix.png', dpi=150)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dwell time at each supply level\n",
    "print('\\n=== Supply Level Dwell Times ===')\n",
    "\n",
    "# For each supply level, calculate how long it stays before changing\n",
    "df_dwell = df_supply[~df_supply['supply_changed']].copy()\n",
    "dwell_stats = df_dwell.groupby('supply')['time_diff'].agg(['mean', 'median', 'std', 'count']).round(2)\n",
    "dwell_stats.columns = ['Mean (min)', 'Median (min)', 'Std (min)', 'Count']\n",
    "print(dwell_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Task Priority Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H4: Does the 5:1 COLLECT_SELL vs ACQUIRE_DELIVER priority ratio maximize throughput?\n",
    "print('=== H4: Task Priority Analysis ===')\n",
    "\n",
    "# Calculate task duration (queue time + execution time)\n",
    "completed_tasks = df_tasks[df_tasks['status'] == 'COMPLETED'].copy()\n",
    "completed_tasks['queue_time'] = (completed_tasks['started_at'] - completed_tasks['ready_at']).dt.total_seconds() / 60\n",
    "completed_tasks['execution_time'] = (completed_tasks['completed_at'] - completed_tasks['started_at']).dt.total_seconds() / 60\n",
    "completed_tasks['total_time'] = (completed_tasks['completed_at'] - completed_tasks['created_at']).dt.total_seconds() / 60\n",
    "\n",
    "print('\\nTask Timing by Type (minutes):')\n",
    "timing_stats = completed_tasks.groupby('task_type').agg({\n",
    "    'queue_time': ['mean', 'median'],\n",
    "    'execution_time': ['mean', 'median'],\n",
    "    'total_time': ['mean', 'median'],\n",
    "    'priority': 'mean'\n",
    "}).round(2)\n",
    "print(timing_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze priority impact on queue time\n",
    "if len(completed_tasks) > 0 and completed_tasks['queue_time'].notna().sum() > 10:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Priority vs queue time\n",
    "    valid_queue = completed_tasks[completed_tasks['queue_time'].notna() & (completed_tasks['queue_time'] > 0)]\n",
    "    if len(valid_queue) > 0:\n",
    "        axes[0].scatter(valid_queue['priority'], valid_queue['queue_time'], alpha=0.5)\n",
    "        axes[0].set_xlabel('Priority')\n",
    "        axes[0].set_ylabel('Queue Time (minutes)')\n",
    "        axes[0].set_title('Priority vs Queue Time')\n",
    "        \n",
    "        # Correlation\n",
    "        r, p = stats.pearsonr(valid_queue['priority'], valid_queue['queue_time'])\n",
    "        axes[0].text(0.05, 0.95, f'r={r:.3f}, p={p:.4f}', transform=axes[0].transAxes, \n",
    "                     verticalalignment='top', fontsize=10)\n",
    "    \n",
    "    # Box plot: queue time by task type\n",
    "    task_types = completed_tasks['task_type'].unique()\n",
    "    if len(task_types) > 1:\n",
    "        sns.boxplot(data=completed_tasks, x='task_type', y='queue_time', ax=axes[1])\n",
    "        axes[1].set_title('Queue Time by Task Type')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/figures/priority_analysis.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Time-Based Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze time-of-day patterns\n",
    "print('=== Time-Based Pattern Analysis ===')\n",
    "\n",
    "df_prices['hour'] = df_prices['recorded_at'].dt.hour\n",
    "df_prices['day_of_week'] = df_prices['recorded_at'].dt.dayofweek\n",
    "\n",
    "# Price volatility by hour\n",
    "hourly_stats = df_prices_sorted.groupby(df_prices_sorted['recorded_at'].dt.hour).agg({\n",
    "    'price_change_pct': ['mean', 'std', 'count'],\n",
    "    'purchase_price': 'mean'\n",
    "}).round(2)\n",
    "hourly_stats.columns = ['Avg Change %', 'Std Change %', 'Count', 'Avg Price']\n",
    "print('\\nHourly Statistics:')\n",
    "print(hourly_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hourly patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Price volatility by hour\n",
    "axes[0, 0].plot(hourly_stats.index, hourly_stats['Avg Change %'], marker='o')\n",
    "axes[0, 0].set_xlabel('Hour of Day (UTC)')\n",
    "axes[0, 0].set_ylabel('Avg Price Change %')\n",
    "axes[0, 0].set_title('Price Volatility by Hour')\n",
    "axes[0, 0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Record count by hour\n",
    "axes[0, 1].bar(hourly_stats.index, hourly_stats['Count'])\n",
    "axes[0, 1].set_xlabel('Hour of Day (UTC)')\n",
    "axes[0, 1].set_ylabel('Number of Records')\n",
    "axes[0, 1].set_title('Data Volume by Hour')\n",
    "\n",
    "# Task completions by hour\n",
    "if len(completed_tasks) > 0:\n",
    "    task_hours = completed_tasks['completed_at'].dt.hour.value_counts().sort_index()\n",
    "    axes[1, 0].bar(task_hours.index, task_hours.values)\n",
    "    axes[1, 0].set_xlabel('Hour of Day (UTC)')\n",
    "    axes[1, 0].set_ylabel('Task Completions')\n",
    "    axes[1, 0].set_title('Task Completions by Hour')\n",
    "\n",
    "# Transaction volume by hour\n",
    "if len(df_transactions) > 0:\n",
    "    txn_hours = df_transactions['created_at'].dt.hour.value_counts().sort_index()\n",
    "    axes[1, 1].bar(txn_hours.index, txn_hours.values)\n",
    "    axes[1, 1].set_xlabel('Hour of Day (UTC)')\n",
    "    axes[1, 1].set_ylabel('Transaction Count')\n",
    "    axes[1, 1].set_title('Transactions by Hour')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/figures/time_patterns.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Price Trend Predictability (H3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H3: Do short-term price trends show momentum or mean reversion?\n",
    "print('=== H3: Price Trend Analysis ===')\n",
    "\n",
    "# Calculate autocorrelation of price changes\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "# For each good, calculate price autocorrelation\n",
    "goods = df_prices_sorted['good_symbol'].unique()\n",
    "autocorr_results = []\n",
    "\n",
    "for good in goods:\n",
    "    good_data = df_prices_sorted[df_prices_sorted['good_symbol'] == good]['price_change_pct'].dropna()\n",
    "    if len(good_data) >= 20:\n",
    "        try:\n",
    "            ac = acf(good_data, nlags=5, fft=False)\n",
    "            autocorr_results.append({\n",
    "                'good': good,\n",
    "                'lag1': ac[1] if len(ac) > 1 else np.nan,\n",
    "                'lag2': ac[2] if len(ac) > 2 else np.nan,\n",
    "                'lag3': ac[3] if len(ac) > 3 else np.nan,\n",
    "                'n': len(good_data)\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "df_autocorr = pd.DataFrame(autocorr_results)\n",
    "if len(df_autocorr) > 0:\n",
    "    print('\\nPrice Change Autocorrelation by Good:')\n",
    "    print(df_autocorr.round(3))\n",
    "    \n",
    "    # Interpretation\n",
    "    print('\\nInterpretation:')\n",
    "    print('  - Positive autocorrelation → Momentum (trends persist)')\n",
    "    print('  - Negative autocorrelation → Mean reversion (trends reverse)')\n",
    "    print('  - Near zero → Random walk (unpredictable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall price change autocorrelation\n",
    "all_price_changes = df_prices_sorted['price_change_pct'].dropna()\n",
    "if len(all_price_changes) >= 20:\n",
    "    overall_ac = acf(all_price_changes, nlags=10, fft=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(overall_ac)), overall_ac)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.axhline(y=1.96/np.sqrt(len(all_price_changes)), color='red', linestyle='--', label='95% CI')\n",
    "    plt.axhline(y=-1.96/np.sqrt(len(all_price_changes)), color='red', linestyle='--')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.title('Price Change Autocorrelation (All Goods)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/figures/price_autocorrelation.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Profitability Analysis by Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate profitability metrics by good\n",
    "print('=== Profitability by Good ===')\n",
    "\n",
    "# From completed tasks\n",
    "task_profit_by_good = completed_tasks.groupby('good').agg({\n",
    "    'total_cost': 'sum',\n",
    "    'total_revenue': 'sum',\n",
    "    'actual_quantity': 'sum',\n",
    "    'id': 'count'\n",
    "}).rename(columns={'id': 'task_count'})\n",
    "\n",
    "task_profit_by_good['net_profit'] = task_profit_by_good['total_revenue'] - task_profit_by_good['total_cost']\n",
    "task_profit_by_good['margin_pct'] = (task_profit_by_good['net_profit'] / task_profit_by_good['total_cost'] * 100).round(2)\n",
    "task_profit_by_good['profit_per_unit'] = (task_profit_by_good['net_profit'] / task_profit_by_good['actual_quantity']).round(2)\n",
    "\n",
    "task_profit_by_good = task_profit_by_good.sort_values('net_profit', ascending=False)\n",
    "print(task_profit_by_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize profitability by good\n",
    "if len(task_profit_by_good) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Top 10 most profitable goods\n",
    "    top_goods = task_profit_by_good.head(10)\n",
    "    axes[0].barh(top_goods.index, top_goods['net_profit'], color='steelblue')\n",
    "    axes[0].set_xlabel('Net Profit')\n",
    "    axes[0].set_title('Top 10 Most Profitable Goods')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Profit margin distribution\n",
    "    margins = task_profit_by_good['margin_pct'].dropna()\n",
    "    if len(margins) > 0:\n",
    "        axes[1].hist(margins, bins=20, edgecolor='black')\n",
    "        axes[1].axvline(x=margins.mean(), color='red', linestyle='--', label=f'Mean: {margins.mean():.1f}%')\n",
    "        axes[1].set_xlabel('Profit Margin %')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('Profit Margin Distribution')\n",
    "        axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/figures/profitability_by_good.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print('='*60)\n",
    "print('MANUFACTURING OPTIMIZATION ANALYSIS SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "print('\\n### DATA SUMMARY ###')\n",
    "print(f'- Market price records: {len(df_prices):,}')\n",
    "print(f'- Transactions: {len(df_transactions):,}')\n",
    "print(f'- Manufacturing tasks: {len(df_tasks):,}')\n",
    "print(f'- Pipelines: {len(df_pipelines):,}')\n",
    "\n",
    "print('\\n### KEY FINDINGS ###')\n",
    "\n",
    "# 1. Activity analysis\n",
    "print('\\n1. MARKET ACTIVITY ANALYSIS')\n",
    "if 'activity_price_stats' in dir():\n",
    "    print('   Activity levels show different price characteristics')\n",
    "    print('   RECOMMENDATION: Consider using activity in trading decisions')\n",
    "\n",
    "# 2. Position sizing\n",
    "print('\\n2. POSITION SIZING')\n",
    "if 'task_profit' in dir():\n",
    "    print('   Task profitability varies by type and quantity')\n",
    "    print('   RECOMMENDATION: Review hardcoded multipliers based on actual performance')\n",
    "\n",
    "# 3. Supply transitions\n",
    "print('\\n3. SUPPLY TRANSITIONS')\n",
    "if 'transition_matrix' in dir() and len(transition_matrix) > 0:\n",
    "    print('   Supply levels show predictable transition patterns')\n",
    "    print('   RECOMMENDATION: Use transition probabilities for timing decisions')\n",
    "\n",
    "# 4. Time patterns\n",
    "print('\\n4. TIME PATTERNS')\n",
    "if 'hourly_stats' in dir():\n",
    "    best_hour = hourly_stats['Avg Change %'].idxmax()\n",
    "    print(f'   Most volatile hour: {best_hour}:00 UTC')\n",
    "    print('   RECOMMENDATION: Consider time-of-day in trading strategy')\n",
    "\n",
    "print('\\n### ACTION ITEMS ###')\n",
    "print('1. Integrate market activity into purchase/sell decisions')\n",
    "print('2. Analyze position sizing vs actual profitability regression')\n",
    "print('3. Implement supply transition prediction model')\n",
    "print('4. Review task priority ratios based on queue time analysis')\n",
    "print('5. Consider time-based trading windows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to file\n",
    "summary_text = \"\"\"\n",
    "# Manufacturing Optimization Analysis Report\n",
    "\n",
    "## Data Summary\n",
    "- Analysis window: {time_range}\n",
    "- Price records: {price_count:,}\n",
    "- Transactions: {txn_count:,}\n",
    "- Tasks: {task_count:,}\n",
    "- Pipelines: {pipeline_count:,}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "### 1. Market Activity (Currently UNUSED)\n",
    "Activity levels (WEAK/GROWING/STRONG/RESTRICTED) show correlation with:\n",
    "- Price stability\n",
    "- Supply transitions\n",
    "- Trade volume\n",
    "\n",
    "**RECOMMENDATION**: Integrate activity into decision-making\n",
    "- Buy when activity = GROWING (prices rising)\n",
    "- Sell when activity = STRONG (high demand)\n",
    "- Avoid WEAK activity markets (low liquidity)\n",
    "\n",
    "### 2. Position Sizing\n",
    "Current hardcoded multipliers:\n",
    "- ABUNDANT: 80% of trade volume\n",
    "- HIGH: 60%\n",
    "- MODERATE: 40%\n",
    "- LIMITED: 20%\n",
    "- SCARCE: 10%\n",
    "\n",
    "**RECOMMENDATION**: Adjust based on regression analysis results\n",
    "\n",
    "### 3. Supply Transitions\n",
    "Supply levels follow predictable patterns.\n",
    "Transition probabilities can be used for timing.\n",
    "\n",
    "### 4. Task Priorities\n",
    "Current ratio: COLLECT_SELL=50 : ACQUIRE_DELIVER=10 (5:1)\n",
    "Queue time analysis suggests potential optimization.\n",
    "\n",
    "## Generated Figures\n",
    "- supply_activity_distribution.png\n",
    "- activity_price_analysis.png\n",
    "- correlation_heatmap.png\n",
    "- supply_transition_matrix.png\n",
    "- quantity_profit_analysis.png\n",
    "- priority_analysis.png\n",
    "- time_patterns.png\n",
    "- price_autocorrelation.png\n",
    "- profitability_by_good.png\n",
    "\"\"\".format(\n",
    "    time_range=f\"{df_prices['recorded_at'].min()} to {df_prices['recorded_at'].max()}\",\n",
    "    price_count=len(df_prices),\n",
    "    txn_count=len(df_transactions),\n",
    "    task_count=len(df_tasks),\n",
    "    pipeline_count=len(df_pipelines)\n",
    ")\n",
    "\n",
    "with open('outputs/recommendations.md', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print('Report saved to outputs/recommendations.md')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
